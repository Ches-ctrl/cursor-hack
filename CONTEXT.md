Im looking at building a hackathon scraper for a 2 hour cursor hackathon tonight.

How can i build this? I want it to be practical and impressive. These are the requiremennts

Hack Scraper

## To Do
- [ ] Lu.ma/ai - scrape (in progress)
	- [ ] Output with links
	- [ ] Add date (from main / individual page)
	- [ ] Scrolling / pagination / load more
- [ ] SERP API - scrape
- [ ] Lu.ma Search Bar
	- [ ] Check for location
- [ ] Browser Instance - setup
- [ ] Cron Job - deploy
- [ ] MCP - setup

## Extensions
- [ ] Firecrawl - setup
- [ ] Agent - setup
- [ ] Proxy - setup
- [ ] Output to CSV
- [ ] Output to Google Sheets
- [ ] WhatsApp notifier / integration

## Sources of hacks
- [ ] Devpost
- [ ] Hackathons UK
- [ ] Eventbrite
- [ ] lu.ma
	- [ ] AI Calendar - https://lu.ma/ai
	- [ ] Subscribed Calendars
	- [ ] SERP API
- [ ] Twitter
- [ ] Dorahacks
- [ ] lablab.ai
- [ ] Automate this...

I need the system design diagram, example code, pseudocode, libraries. It needs to be deployed.

Provide staged options so i can build this progressively. I will need to run a browser and proxy for some sites


ChatGPT said:

Thought for 22 seconds
Hackathon Scraper Design
Iâ€™ve dropped a full blueprint into the canvasâ€”architecture diagram, staged roadmap, key code snippets, and deployment notesâ€”all ready for a rapid two-hour build.

Next step ideas

Pick a stack â€“ the sample code is Python + Playwright. Happy to switch to Node/Puppeteer if you prefer.

Clone the scaffold â€“ let me know if youâ€™d like a bare-bones repo template.

Focus your two hours â€“ Stage 1 (Lu.ma) â†’ Stage 3 (CSV/Sheets) is usually doable in time.

What language/runtime are you leaning toward, and do you want any part of the code expanded (e.g., Devpost handler or Google Sheets auth)?






You said:
Python / playwright


ChatGPT said:

Thought for 28 seconds
Great â€” Python + Playwright is exactly what the blueprint assumed, so you can dive straight in. Below is a step-by-step starter kit that turns the canvas plan into runnable code. Feel free to copy-paste; everything is â€œhappy-pathâ€ and trimmed for a two-hour sprint.

1. Quick project scaffold
text
Copy
Edit
hack-scraper/
â”œâ”€â”€ hack_scraper/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ cli.py               # Typer entry-point
â”‚   â”œâ”€â”€ aggregator.py
â”‚   â”œâ”€â”€ exporter.py
â”‚   â””â”€â”€ scraper/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base.py
â”‚       â”œâ”€â”€ luma.py
â”‚       â”œâ”€â”€ devpost.py
â”‚       â””â”€â”€ eventbrite.py
â”œâ”€â”€ output/                  # CSVs here (git-ignored)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ .env                     # secrets (git-ignored)
requirements.txt
makefile
Copy
Edit
playwright==1.44.*
pandas
typer[all]
python-dotenv
python-dateutil
beautifulsoup4
httpx
gspread
google-auth
twilio
schedule
Dockerfile
dockerfile
Copy
Edit
FROM python:3.12-slim
WORKDIR /app

# 1) deps  2) install browser  3) copy code
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt \
 && playwright install --with-deps chromium
COPY . .

CMD ["python", "-m", "hack_scraper.cli", "run"]
Local first run

bash
Copy
Edit
pip install -r requirements.txt
playwright install
python -m hack_scraper.cli scrape luma --limit 150
2. Minimal Typer CLI (hack_scraper/cli.py)
python
Copy
Edit
import asyncio, typer, pandas as pd
from hack_scraper.scraper import get_scraper
from hack_scraper.aggregator import aggregate_and_export

app = typer.Typer(help="Hackathon Scraper CLI")

@app.command()
def scrape(source: str = "luma", limit: int = 300, proxy: str | None = None):
    """Scrape one source and save to output/{source}.csv"""
    scraper = get_scraper(source)(proxy=proxy)
    df: pd.DataFrame = asyncio.run(scraper.scrape(limit))
    out = f"output/{source}.csv"
    df.to_csv(out, index=False)
    typer.echo(f"âœ…  {len(df)} rows âžœ {out}")

@app.command()
def run():
    """Full pipeline: scrape core sources, de-dupe, export."""
    asyncio.run(aggregate_and_export(["luma", "devpost", "eventbrite"]))

if __name__ == "__main__":
    app()
3. Base class and Lu.ma scraper
hack_scraper/scraper/base.py
python
Copy
Edit
from abc import ABC, abstractmethod
import pandas as pd

class BaseScraper(ABC):
    def __init__(self, proxy: str | None = None):
        self.proxy = proxy

    @abstractmethod
    async def scrape(self, limit: int) -> pd.DataFrame: ...
hack_scraper/scraper/luma.py
python
Copy
Edit
import datetime as dt, pandas as pd
from playwright.async_api import async_playwright
from .base import BaseScraper

LUMA_URL = "https://lu.ma/ai"

class LumaScraper(BaseScraper):
    async def scrape(self, limit: int = 300) -> pd.DataFrame:
        events, seen = [], set()

        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                proxy={"server": self.proxy} if self.proxy else None,
            )
            page = await browser.new_page()
            await page.goto(LUMA_URL, timeout=60_000)
            await page.wait_for_selector('[itemprop="event"]')

            while len(events) < limit:
                cards = await page.query_selector_all('[itemprop="event"]')
                for card in cards:
                    url = await card.eval_on_selector("a", "e=>e.href")
                    if url in seen: continue
                    seen.add(url)

                    title = await card.eval_on_selector("h3", "e=>e.textContent")
                    iso  = await card.eval_on_selector("time", "e=>e.dateTime")
                    events.append({
                        "title": title.strip(),
                        "url": url,
                        "date": dt.datetime.fromisoformat(iso).date(),
                        "source": "luma"
                    })

                await page.mouse.wheel(0, 2000)
                await page.wait_for_timeout(1200)
                if await page.is_visible("text=No more"): break

            await browser.close()

        return pd.DataFrame(events)
hack_scraper/scraper/__init__.py
python
Copy
Edit
from .luma import LumaScraper
# from .devpost import DevpostScraper
# from .eventbrite import EventbriteScraper

def get_scraper(name: str):
    return {
        "luma": LumaScraper,
        # "devpost": DevpostScraper,
        # "eventbrite": EventbriteScraper,
    }[name.lower()]
(Devpost & Eventbrite can start as static-HTML parsers with httpx+BeautifulSoupâ€”copy the pattern later.)

4. Aggregator & exports
hack_scraper/aggregator.py
python
Copy
Edit
import pandas as pd, asyncio, os, json
from pathlib import Path
from hack_scraper.scraper import get_scraper
from hack_scraper.exporter import export_csv, export_google_sheet, notify_whatsapp

OUTPUT = Path("output")

async def scrape_all(sources: list[str]) -> list[pd.DataFrame]:
    tasks = [get_scraper(src)().scrape(500) for src in sources]
    return await asyncio.gather(*tasks)

async def aggregate_and_export(sources: list[str]):
    dfs = await scrape_all(sources)
    df = pd.concat(dfs).drop_duplicates(subset="url").sort_values("date")
    OUTPUT.mkdir(exist_ok=True)
    master = OUTPUT / "events.csv"
    export_csv(df, master)

    # Optional extras
    if os.getenv("GSHEET_ID"):         export_google_sheet(df)
    if os.getenv("TWILIO_SID"):        notify_whatsapp(df.head(5))
hack_scraper/exporter.py
python
Copy
Edit
import pandas as pd, os, json
from twilio.rest import Client
import gspread
from google.oauth2.service_account import Credentials

def export_csv(df: pd.DataFrame, path):
    df.to_csv(path, index=False)
    print(f"ðŸ“„ CSV updated -> {path}")

def export_google_sheet(df: pd.DataFrame):
    creds = Credentials.from_service_account_info(
        json.loads(os.getenv("GS_JSON")),
        scopes=["https://www.googleapis.com/auth/spreadsheets"]
    )
    gc = gspread.authorize(creds)
    sh = gc.open_by_key(os.getenv("GSHEET_ID"))
    sh.values_clear("Sheet1")
    sh.values_update(
        "Sheet1",
        params={"valueInputOption": "RAW"},
        body={"values": [df.columns.tolist()] + df.values.tolist()}
    )
    print("âœ… Google Sheet updated")

def notify_whatsapp(df_head: pd.DataFrame):
    msg = "\n".join(f"â€¢ {r.title} â€“ {r.date}" for r in df_head.itertuples())
    client = Client(os.getenv("TWILIO_SID"), os.getenv("TWILIO_TOKEN"))
    client.messages.create(
        body=f"New hackathons:\n{msg}",
        from_="whatsapp:+14155238886",
        to=os.getenv("MY_WHATSAPP")
    )
    print("ðŸ“² WhatsApp ping sent")
5. .env template
ini
Copy
Edit
# Playwright proxy (omit if none)
PROXY=http://user:pass@host:port

# SERP API (Stage 2+)
SERP_API_KEY=...

# Google Sheets (Stage 3)
GS_JSON=<<one-line JSON>>
GSHEET_ID=xxxxxxxxxxxxxxxxxxxxx

# Twilio (Stage 4)
TWILIO_SID=ACxxxxxxxxxxxx
TWILIO_TOKEN=xxxxxxxxxxxx
MY_WHATSAPP=+447700900000
6. GitHub Actions cron (copy to .github/workflows/scrape.yml)
yaml
Copy
Edit
name: Hack Scraper

on:
  schedule:
    - cron: "0 */6 * * *"       # every 6 h
  workflow_dispatch:            # manual run

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: "3.12" }
      - run: pip install -r requirements.txt
      - run: |
          playwright install --with-deps chromium
          python -m hack_scraper.cli run
        env: ${{ secrets }}      # map repo secrets straight to env
(Render.com or Fly.io one-offs work the sameâ€”build the Dockerfile then add a cron schedule.)

7. Two-hour game-plan
â°	Task	Outcome
0â€“10 min	Clone scaffold, create virtual-env, install deps, set .env	Local python -m hack_scraper.cli scrape luma returns CSV
10â€“45 min	Flesh out Lu.ma scraper (above code)	output/luma.csv with 100-200 rows
45â€“70 min	Add Devpost & Eventbrite (static HTML)	CSVs for each, aggregator merges
70â€“90 min	Implement exporter.py CSV + Sheets	output/events.csv and sheet populated
90â€“110 min	Dockerfile & docker run works; optionally push to GitHub	Container tested locally
110â€“120 min	Wire GitHub Action or Render cron	End-to-end scheduled run

What next?
âœ… Ready to code? Copy the snippets above and run uvicorn style dev-loop.

â“ Need help with Devpost HTML parsing or Google auth JSON? Just shout.

ðŸš€ Want Firecrawl/agent-style autonomous runs? We can bolt that on after the live demo.

Ping me with whichever bit youâ€™d like expanded or debugged, and happy hacking! ðŸ§€







You said:
I want firecrawl as an option and also will need to run a browser eg browserbase


ChatGPT said:

Thought for 26 seconds
Hackathon Scraper Design

Hack Scraper â€“ 2â€‘hour Build Plan
Goal: Collect upcoming hackathons & AIâ€‘events from multiple public sources, export them, and optionally push notifications â€“ all in a single evening sprint.

1. Highâ€‘Level Architecture
graph TD
  subgraph Scheduler & Runtime
    CRON["Cron / GitHubÂ Action"] --> CONTAINER[Docker Container]
  end

  subgraph Scraper Service
    CONTAINER --> PWR[PlaywrightÂ + Proxy]
    PWR --> LUMA[Lu.ma Handler]
    PWR --> DEV[Devpost Handler]
    PWR --> EVT[Eventbrite Handler]
    PWR --> SERP[SERPâ€‘API Handler]
  end

  LUMA --> Q[(Queue / Channel)]
  DEV  --> Q
  EVT  --> Q
  SERP --> Q

  Q --> AGG[Aggregator & Deâ€‘duper]
  AGG --> DB[(SQLite / Postgres)]
  AGG --> CSV[CSV Export]
  AGG --> GS[GoogleÂ Sheets API]
  AGG --> WA[WhatsApp via Twilio]
Everything lives in one repo with a docker-compose.yml so you can hack locally and deploy the same image on Render/Fly/Heroku.

2. Staged Roadmap
Stage	Time	Milestone	What you ship
0	10Â min	Scaffold	Git repo, virtualâ€‘env/Poetry, Dockerfile, .env for keys, basic Typer CLI skeleton.
1	40Â min	Lu.ma AI Calendar	Playwright scraper with infinite scroll ðŸ”„, date parsing, exports CSV to /output/luma.csv.
2	30Â min	More Sources	Add Devpost & Eventbrite (simple HTML + query param filters). Introduce aggregator & SQLite.
3	20Â min	Outputs	Merge â†’ events.csv, push to Google Sheets via service account.
4	10Â min	Proxy & Headless Browser	Swap Playwright launch args for proxy + stealth; parameterise in .env.
5	10Â min	Cron / Deploy	GitHub Action on schedule or crontab inside container. (e.g. every 6Â h). Optionally push a WhatsApp summary via Twilio.
(If you still have juice, StageÂ 6 = Firecrawl / Agents.)

3. Core Libraries
Need	Python pkg	Why
Headless browser	playwright (fast), fallback selenium-stealth	Handles JS + infinite scroll
HTTP / fallback	httpx, beautifulsoup4	For static sites
Data wrangling	pandas, python-dateutil	Easy export & date math
Google Sheets	gspread + googleâ€‘auth	Sheets API
Messaging	twilio	WhatsApp/SMS push
CLI	typer	Nice DX
Env management	python-dotenv	Secrets in .env
Scheduling	schedule or OS cron	Local runs
Testing	pytest	Quick sanity
4. Example Code Snippets
4.1 Lu.ma AI Calendar Scraper (Playwright)
from playwright.sync_api import sync_playwright
import pandas as pd, datetime as dt

LUMA_URL = "https://lu.ma/ai"

def scrape_luma(limit: int = 300, proxy: str | None = None) -> pd.DataFrame:
    events = []
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=[f"--proxy-server={proxy}"] if proxy else None,
        )
        page = browser.new_page()
        page.goto(LUMA_URL, timeout=60000)
        page.wait_for_selector('[itemprop="event"]')

        while len(events) < limit:
            cards = page.locator('[itemprop="event"]')
            for i in range(cards.count()):
                card = cards.nth(i)
                href = card.locator('a').get_attribute('href')
                title = card.locator('h3').inner_text()
                iso_date = card.locator('time').get_attribute('datetime')
                date = dt.datetime.fromisoformat(iso_date)
                events.append({"title": title, "url": href, "date": date})
            # trigger more content
            page.mouse.wheel(0, 2000)
            page.wait_for_timeout(1200)
            if page.locator('text="No more"').count():
                break
        browser.close()
    return pd.DataFrame(events)
4.2 SERPâ€‘API Query
import os, requests, pandas as pd

def google_hackathons(query="hackathon", location=None, api_key=os.getenv("SERP_API_KEY")):
    params = {
        "engine": "google",
        "q": f"{query} site:lu.ma | site:eventbrite.com | site:devpost.com",
        "api_key": api_key,
        "num": 100,
    }
    if location:
        params["location"] = location
    res = requests.get("https://serpapi.com/search", params=params, timeout=15).json()
    return pd.DataFrame(res.get("organic_results", []))
4.3 Aggregator Pseudocode
for source in sources:
    df = source.scrape()
    upsert(df)  # keyed on canonical_url

rows = db.read("SELECT * FROM events WHERE date >= today")
export_csv(rows)
export_google_sheets(rows)
if new_items:
    send_whatsapp(summary(new_items))
5. Deployment Cheatsheet
Dockerfile (python:3.12â€‘slim) installs system deps: playwright install chromium.

dockerâ€‘compose.yml â€“ mount /output for CSV & .env secrets.

Render.com: oneâ€‘off cron job â†’ python main.py run.

GitHub Actions: workflow cron â†’ checkout â†’ setupâ€‘python â†’ pip install -r reqs â†’ python main.py run.

Fly.io / Heroku as PlanÂ B.

6. Proxy & Stealth Notes
Use BrightData or ProxyMesh â†’ store creds in .env.

In Playwright: launch(args=["--proxy-server=http://user:pass@host:port"]).

For truly nasty sites add playwright-stealth.

7. WhatsApp Notifier (Twilio)
from twilio.rest import Client

def notify_whatsapp(msg: str):
    client = Client(os.getenv('TWILIO_SID'), os.getenv('TWILIO_TOKEN'))
    client.messages.create(
        body=msg,
        from_='whatsapp:+14155238886',
        to=os.getenv('MY_WHATSAPP')
    )
Call this from the aggregator whenever new_items is nonâ€‘empty.

8. Reference Commandâ€‘Line
# Run scrape locally
python main.py scrape --source luma --limit 200

# Aggregate & export
python main.py aggregate --sheet hacks

# Full pipeline (for cron)
python main.py run
9. Firecrawl & Browserbase (optional boltâ€‘ons)
9.1Â Firecrawl
When to useÂ â€“ lightningâ€‘fast static scraping when you donâ€™t need full JS rendering.

InstallÂ pip install firecrawl

EnvÂ FIRECRAWL_API_KEY=<token>

from firecrawl import Firecrawl
import os

fc = Firecrawl(api_key=os.getenv("FIRECRAWL_API_KEY"))

def fc_scrape(url: str):
    data = fc.crawl(url, extract=["title", "html", "images", "date_published"])
    return {
        "title": data.get("title"),
        "url": url,
        "date": data.get("date_published"),
        "source": "firecrawl",
    }
Wrap this in a FirecrawlScraper(BaseScraper) and register it in scraper/__init__.py so --mode firecrawl works across any URL list.

9.2Â Browserbase â€“ remote Chromium
WhyÂ â€“ offâ€‘load CPU/RAM, evade local network limits, scale parallel tabs.

Install clientÂ pip install browserbase

Env

BROWSERBASE_API_KEY

BROWSERBASE_REGION (optional)

from browserbase import BrowserBase
from playwright.async_api import async_playwright
import os, asyncio

async def bb_launch():
    bb = BrowserBase(os.getenv("BROWSERBASE_API_KEY"))
    ws_endpoint = bb.create_session().get("ws_endpoint")
    p = await async_playwright().start()
    browser = await p.chromium.connect_over_cdp(ws_endpoint)
    return browser
Add a USE_BROWSERBASE=true guard inside BaseScraper so you transparently swap launch() for connect_over_cdp.

9.3Â Runtime modes summary
Mode	Trigger	Renderer	Best for
local (default)	nothing	Playwright local	fast dev loops
proxy	PROXY= set	Playwright + proxy	geoâ€‘blocked sites
browserbase	USE_BROWSERBASE=true	Remote Chromium	heavy/parallel crawls
firecrawl	FIRECRAWL_API_KEY	Firecrawl API	pure HTML pages
typer CLI addition:

python -m hack_scraper.cli scrape luma --mode browserbase
Use an Enum for --mode and let each scraper decide the launch strategy.

10.Â Good luck & happy hacking! ðŸ§€


Edited

Answer in chat instead
âœ… Added Section 9 to the blueprint with:

Firecrawl integration â€“ install, env var, helper snippet, and how to wrap it as a FirecrawlScraper.

Browserbase remote browser â€“ quick client example and switchable launch logic.

A modes table + CLI flag so you can flip between local, proxy, browserbase, and firecrawl.

Open the canvas to see the full diff, and let me know which piece youâ€™d like fleshed out nextâ€”e.g., wiring the --mode flag, Devpost via Firecrawl, or Browserbase parallelization.

# Hack Scraper â€“ 2â€‘hour Build Plan

> **Goal:** Collect upcoming hackathons & AIâ€‘events from multiple public sources, export them, and optionally push notifications â€“ all in a single evening sprint.

---

## 1. Highâ€‘Level Architecture

```mermaid
graph TD
  subgraph Scheduler & Runtime
    CRON["Cron / GitHubÂ Action"] --> CONTAINER[Docker Container]
  end

  subgraph Scraper Service
    CONTAINER --> PWR[PlaywrightÂ + Proxy]
    PWR --> LUMA[Lu.ma Handler]
    PWR --> DEV[Devpost Handler]
    PWR --> EVT[Eventbrite Handler]
    PWR --> SERP[SERPâ€‘API Handler]
  end

  LUMA --> Q[(Queue / Channel)]
  DEV  --> Q
  EVT  --> Q
  SERP --> Q

  Q --> AGG[Aggregator & Deâ€‘duper]
  AGG --> DB[(SQLite / Postgres)]
  AGG --> CSV[CSV Export]
  AGG --> GS[GoogleÂ Sheets API]
  AGG --> WA[WhatsApp via Twilio]
```

*Everything lives in **one repo** with a `docker-compose.yml` so you can hack locally and deploy the same image on Render/Fly/Heroku.*

---

## 2. Staged Roadmap

| Stage | Time   | Milestone                    | What you ship                                                                                                             |
| ----- | ------ | ---------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| **0** | 10Â min | **Scaffold**                 | Git repo, virtualâ€‘env/Poetry, Dockerfile, `.env` for keys, basic `Typer` CLI skeleton.                                    |
| **1** | 40Â min | **Lu.ma AI Calendar**        | Playwright scraper with infinite scroll ðŸ”„, date parsing, exports CSV to `/output/luma.csv`.                              |
| **2** | 30Â min | **More Sources**             | Add Devpost & Eventbrite (simple HTML + query param filters). Introduce aggregator & SQLite.                              |
| **3** | 20Â min | **Outputs**                  | Merge â†’ `events.csv`, push to Google Sheets via service account.                                                          |
| **4** | 10Â min | **Proxy & Headless Browser** | Swap Playwright launch args for proxy + stealth; parameterise in `.env`.                                                  |
| **5** | 10Â min | **Cron / Deploy**            | GitHub Action on schedule or `crontab` inside container. (e.g. every 6Â h). Optionally push a WhatsApp summary via Twilio. |

*(If you still have juice, StageÂ 6 = Firecrawl / Agents.)*

---

## 3. Core Libraries

| Need             | Python pkg                                       | Why                          |
| ---------------- | ------------------------------------------------ | ---------------------------- |
| Headless browser | `playwright` (fast), fallback `selenium-stealth` | Handles JS + infinite scroll |
| HTTP / fallback  | `httpx`, `beautifulsoup4`                        | For static sites             |
| Data wrangling   | `pandas`, `python-dateutil`                      | Easy export & date math      |
| Google Sheets    | `gspread` + `googleâ€‘auth`                        | Sheets API                   |
| Messaging        | `twilio`                                         | WhatsApp/SMS push            |
| CLI              | `typer`                                          | Nice DX                      |
| Env management   | `python-dotenv`                                  | Secrets in .env              |
| Scheduling       | `schedule` or OS cron                            | Local runs                   |
| Testing          | `pytest`                                         | Quick sanity                 |

---

## 4. Example Code Snippets

### 4.1 Lu.ma AI Calendar Scraper (Playwright)

```python
from playwright.sync_api import sync_playwright
import pandas as pd, datetime as dt

LUMA_URL = "https://lu.ma/ai"

def scrape_luma(limit: int = 300, proxy: str | None = None) -> pd.DataFrame:
    events = []
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=[f"--proxy-server={proxy}"] if proxy else None,
        )
        page = browser.new_page()
        page.goto(LUMA_URL, timeout=60000)
        page.wait_for_selector('[itemprop="event"]')

        while len(events) < limit:
            cards = page.locator('[itemprop="event"]')
            for i in range(cards.count()):
                card = cards.nth(i)
                href = card.locator('a').get_attribute('href')
                title = card.locator('h3').inner_text()
                iso_date = card.locator('time').get_attribute('datetime')
                date = dt.datetime.fromisoformat(iso_date)
                events.append({"title": title, "url": href, "date": date})
            # trigger more content
            page.mouse.wheel(0, 2000)
            page.wait_for_timeout(1200)
            if page.locator('text="No more"').count():
                break
        browser.close()
    return pd.DataFrame(events)
```

### 4.2 SERPâ€‘API Query

```python
import os, requests, pandas as pd

def google_hackathons(query="hackathon", location=None, api_key=os.getenv("SERP_API_KEY")):
    params = {
        "engine": "google",
        "q": f"{query} site:lu.ma | site:eventbrite.com | site:devpost.com",
        "api_key": api_key,
        "num": 100,
    }
    if location:
        params["location"] = location
    res = requests.get("https://serpapi.com/search", params=params, timeout=15).json()
    return pd.DataFrame(res.get("organic_results", []))
```

### 4.3 Aggregator Pseudocode

```text
for source in sources:
    df = source.scrape()
    upsert(df)  # keyed on canonical_url

rows = db.read("SELECT * FROM events WHERE date >= today")
export_csv(rows)
export_google_sheets(rows)
if new_items:
    send_whatsapp(summary(new_items))
```

---

## 5. Deployment Cheatsheet

1. **Dockerfile** (python:3.12â€‘slim) installs system deps: `playwright install chromium`.
2. **dockerâ€‘compose.yml** â€“ mount `/output` for CSV & `.env` secrets.
3. **Render.com**: oneâ€‘off cron job â†’ `python main.py run`.
4. **GitHub Actions**: workflow cron â†’ checkout â†’ setupâ€‘python â†’ `pip install -r reqs` â†’ `python main.py run`.
5. **Fly.io / Heroku** as PlanÂ B.

---

## 6. Proxy & Stealth Notes

* Use **BrightData** or **ProxyMesh** â†’ store creds in `.env`.
* In Playwright: `launch(args=["--proxy-server=http://user:pass@host:port"])`.
* For truly nasty sites add [`playwright-stealth`](https://github.com/msidorenko/playwright-stealth).

---

## 7. WhatsApp Notifier (Twilio)

```python
from twilio.rest import Client

def notify_whatsapp(msg: str):
    client = Client(os.getenv('TWILIO_SID'), os.getenv('TWILIO_TOKEN'))
    client.messages.create(
        body=msg,
        from_='whatsapp:+14155238886',
        to=os.getenv('MY_WHATSAPP')
    )
```

Call this from the aggregator whenever `new_items` is nonâ€‘empty.

---

## 8. Reference Commandâ€‘Line

```bash
# Run scrape locally
python main.py scrape --source luma --limit 200

# Aggregate & export
python main.py aggregate --sheet hacks

# Full pipeline (for cron)
python main.py run
```

---

---

## 9. Firecrawl & Browserbase (optional boltâ€‘ons)

### 9.1Â Firecrawl

* **When to use**Â â€“ lightningâ€‘fast static scraping when you donâ€™t need full JS rendering.
* **Install**Â `pip install firecrawl`
* **Env**Â `FIRECRAWL_API_KEY=<token>`

```python
from firecrawl import Firecrawl
import os

fc = Firecrawl(api_key=os.getenv("FIRECRAWL_API_KEY"))

def fc_scrape(url: str):
    data = fc.crawl(url, extract=["title", "html", "images", "date_published"])
    return {
        "title": data.get("title"),
        "url": url,
        "date": data.get("date_published"),
        "source": "firecrawl",
    }
```

*Wrap this in a `FirecrawlScraper(BaseScraper)` and register it in `scraper/__init__.py` so `--mode firecrawl` works across any URL list.*

---

### 9.2Â Browserbase â€“ remote Chromium

* **Why**Â â€“ offâ€‘load CPU/RAM, evade local network limits, scale parallel tabs.
* **Install client**Â `pip install browserbase`
* **Env**

  * `BROWSERBASE_API_KEY`
  * `BROWSERBASE_REGION` (optional)

```python
from browserbase import BrowserBase
from playwright.async_api import async_playwright
import os, asyncio

async def bb_launch():
    bb = BrowserBase(os.getenv("BROWSERBASE_API_KEY"))
    ws_endpoint = bb.create_session().get("ws_endpoint")
    p = await async_playwright().start()
    browser = await p.chromium.connect_over_cdp(ws_endpoint)
    return browser
```

*Add a `USE_BROWSERBASE=true` guard inside `BaseScraper` so you transparently swap `launch()` for `connect_over_cdp`.*

---

### 9.3Â Runtime modes summary

| Mode                | Trigger                | Renderer           | Best for              |
| ------------------- | ---------------------- | ------------------ | --------------------- |
| **local** (default) | nothing                | Playwright local   | fast dev loops        |
| **proxy**           | `PROXY=` set           | Playwright + proxy | geoâ€‘blocked sites     |
| **browserbase**     | `USE_BROWSERBASE=true` | Remote Chromium    | heavy/parallel crawls |
| **firecrawl**       | `FIRECRAWL_API_KEY`    | Firecrawl API      | pure HTML pages       |

`typer` CLI addition:

```bash
python -m hack_scraper.cli scrape luma --mode browserbase
```

Use an `Enum` for `--mode` and let each scraper decide the launch strategy.

---

### 10.Â Good luck & happy hacking! ðŸ§€
