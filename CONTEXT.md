Im looking at building a hackathon scraper for a 2 hour cursor hackathon tonight.

How can i build this? I want it to be practical and impressive. These are the requiremennts

Hack Scraper

## To Do
- [ ] Lu.ma/ai - scrape (in progress)
	- [ ] Output with links
	- [ ] Add date (from main / individual page)
	- [ ] Scrolling / pagination / load more
- [ ] SERP API - scrape
- [ ] Lu.ma Search Bar
	- [ ] Check for location
- [ ] Browser Instance - setup
- [ ] Cron Job - deploy
- [ ] MCP - setup

## Extensions
- [ ] Firecrawl - setup
- [ ] Agent - setup
- [ ] Proxy - setup
- [ ] Output to CSV
- [ ] Output to Google Sheets
- [ ] WhatsApp notifier / integration

## Sources of hacks
- [ ] Devpost
- [ ] Hackathons UK
- [ ] Eventbrite
- [ ] lu.ma
	- [ ] AI Calendar - https://lu.ma/ai
	- [ ] Subscribed Calendars
	- [ ] SERP API
- [ ] Twitter
- [ ] Dorahacks
- [ ] lablab.ai
- [ ] Automate this...

I need the system design diagram, example code, pseudocode, libraries. It needs to be deployed.

Provide staged options so i can build this progressively. I will need to run a browser and proxy for some sites


ChatGPT said:

Thought for 22 seconds
Hackathon Scraper Design
I‚Äôve dropped a full blueprint into the canvas‚Äîarchitecture diagram, staged roadmap, key code snippets, and deployment notes‚Äîall ready for a rapid two-hour build.

Next step ideas

Pick a stack ‚Äì the sample code is Python + Playwright. Happy to switch to Node/Puppeteer if you prefer.

Clone the scaffold ‚Äì let me know if you‚Äôd like a bare-bones repo template.

Focus your two hours ‚Äì Stage 1 (Lu.ma) ‚Üí Stage 3 (CSV/Sheets) is usually doable in time.

What language/runtime are you leaning toward, and do you want any part of the code expanded (e.g., Devpost handler or Google Sheets auth)?






You said:
Python / playwright


ChatGPT said:

Thought for 28 seconds
Great ‚Äî Python + Playwright is exactly what the blueprint assumed, so you can dive straight in. Below is a step-by-step starter kit that turns the canvas plan into runnable code. Feel free to copy-paste; everything is ‚Äúhappy-path‚Äù and trimmed for a two-hour sprint.

1. Quick project scaffold
text
Copy
Edit
hack-scraper/
‚îú‚îÄ‚îÄ hack_scraper/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îú‚îÄ‚îÄ cli.py               # Typer entry-point
‚îÇ   ‚îú‚îÄ‚îÄ aggregator.py
‚îÇ   ‚îú‚îÄ‚îÄ exporter.py
‚îÇ   ‚îî‚îÄ‚îÄ scraper/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ base.py
‚îÇ       ‚îú‚îÄ‚îÄ luma.py
‚îÇ       ‚îú‚îÄ‚îÄ devpost.py
‚îÇ       ‚îî‚îÄ‚îÄ eventbrite.py
‚îú‚îÄ‚îÄ output/                  # CSVs here (git-ignored)
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Dockerfile
‚îî‚îÄ‚îÄ .env                     # secrets (git-ignored)
requirements.txt
makefile
Copy
Edit
playwright==1.44.*
pandas
typer[all]
python-dotenv
python-dateutil
beautifulsoup4
httpx
gspread
google-auth
twilio
schedule
Dockerfile
dockerfile
Copy
Edit
FROM python:3.12-slim
WORKDIR /app

# 1) deps  2) install browser  3) copy code
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt \
 && playwright install --with-deps chromium
COPY . .

CMD ["python", "-m", "hack_scraper.cli", "run"]
Local first run

bash
Copy
Edit
pip install -r requirements.txt
playwright install
python -m hack_scraper.cli scrape luma --limit 150
2. Minimal Typer CLI (hack_scraper/cli.py)
python
Copy
Edit
import asyncio, typer, pandas as pd
from hack_scraper.scraper import get_scraper
from hack_scraper.aggregator import aggregate_and_export

app = typer.Typer(help="Hackathon Scraper CLI")

@app.command()
def scrape(source: str = "luma", limit: int = 300, proxy: str | None = None):
    """Scrape one source and save to output/{source}.csv"""
    scraper = get_scraper(source)(proxy=proxy)
    df: pd.DataFrame = asyncio.run(scraper.scrape(limit))
    out = f"output/{source}.csv"
    df.to_csv(out, index=False)
    typer.echo(f"‚úÖ  {len(df)} rows ‚ûú {out}")

@app.command()
def run():
    """Full pipeline: scrape core sources, de-dupe, export."""
    asyncio.run(aggregate_and_export(["luma", "devpost", "eventbrite"]))

if __name__ == "__main__":
    app()
3. Base class and Lu.ma scraper
hack_scraper/scraper/base.py
python
Copy
Edit
from abc import ABC, abstractmethod
import pandas as pd

class BaseScraper(ABC):
    def __init__(self, proxy: str | None = None):
        self.proxy = proxy

    @abstractmethod
    async def scrape(self, limit: int) -> pd.DataFrame: ...
hack_scraper/scraper/luma.py
python
Copy
Edit
import datetime as dt, pandas as pd
from playwright.async_api import async_playwright
from .base import BaseScraper

LUMA_URL = "https://lu.ma/ai"

class LumaScraper(BaseScraper):
    async def scrape(self, limit: int = 300) -> pd.DataFrame:
        events, seen = [], set()

        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                proxy={"server": self.proxy} if self.proxy else None,
            )
            page = await browser.new_page()
            await page.goto(LUMA_URL, timeout=60_000)
            await page.wait_for_selector('[itemprop="event"]')

            while len(events) < limit:
                cards = await page.query_selector_all('[itemprop="event"]')
                for card in cards:
                    url = await card.eval_on_selector("a", "e=>e.href")
                    if url in seen: continue
                    seen.add(url)

                    title = await card.eval_on_selector("h3", "e=>e.textContent")
                    iso  = await card.eval_on_selector("time", "e=>e.dateTime")
                    events.append({
                        "title": title.strip(),
                        "url": url,
                        "date": dt.datetime.fromisoformat(iso).date(),
                        "source": "luma"
                    })

                await page.mouse.wheel(0, 2000)
                await page.wait_for_timeout(1200)
                if await page.is_visible("text=No more"): break

            await browser.close()

        return pd.DataFrame(events)
hack_scraper/scraper/__init__.py
python
Copy
Edit
from .luma import LumaScraper
# from .devpost import DevpostScraper
# from .eventbrite import EventbriteScraper

def get_scraper(name: str):
    return {
        "luma": LumaScraper,
        # "devpost": DevpostScraper,
        # "eventbrite": EventbriteScraper,
    }[name.lower()]
(Devpost & Eventbrite can start as static-HTML parsers with httpx+BeautifulSoup‚Äîcopy the pattern later.)

4. Aggregator & exports
hack_scraper/aggregator.py
python
Copy
Edit
import pandas as pd, asyncio, os, json
from pathlib import Path
from hack_scraper.scraper import get_scraper
from hack_scraper.exporter import export_csv, export_google_sheet, notify_whatsapp

OUTPUT = Path("output")

async def scrape_all(sources: list[str]) -> list[pd.DataFrame]:
    tasks = [get_scraper(src)().scrape(500) for src in sources]
    return await asyncio.gather(*tasks)

async def aggregate_and_export(sources: list[str]):
    dfs = await scrape_all(sources)
    df = pd.concat(dfs).drop_duplicates(subset="url").sort_values("date")
    OUTPUT.mkdir(exist_ok=True)
    master = OUTPUT / "events.csv"
    export_csv(df, master)

    # Optional extras
    if os.getenv("GSHEET_ID"):         export_google_sheet(df)
    if os.getenv("TWILIO_SID"):        notify_whatsapp(df.head(5))
hack_scraper/exporter.py
python
Copy
Edit
import pandas as pd, os, json
from twilio.rest import Client
import gspread
from google.oauth2.service_account import Credentials

def export_csv(df: pd.DataFrame, path):
    df.to_csv(path, index=False)
    print(f"üìÑ CSV updated -> {path}")

def export_google_sheet(df: pd.DataFrame):
    creds = Credentials.from_service_account_info(
        json.loads(os.getenv("GS_JSON")),
        scopes=["https://www.googleapis.com/auth/spreadsheets"]
    )
    gc = gspread.authorize(creds)
    sh = gc.open_by_key(os.getenv("GSHEET_ID"))
    sh.values_clear("Sheet1")
    sh.values_update(
        "Sheet1",
        params={"valueInputOption": "RAW"},
        body={"values": [df.columns.tolist()] + df.values.tolist()}
    )
    print("‚úÖ Google Sheet updated")

def notify_whatsapp(df_head: pd.DataFrame):
    msg = "\n".join(f"‚Ä¢ {r.title} ‚Äì {r.date}" for r in df_head.itertuples())
    client = Client(os.getenv("TWILIO_SID"), os.getenv("TWILIO_TOKEN"))
    client.messages.create(
        body=f"New hackathons:\n{msg}",
        from_="whatsapp:+14155238886",
        to=os.getenv("MY_WHATSAPP")
    )
    print("üì≤ WhatsApp ping sent")
5. .env template
ini
Copy
Edit
# Playwright proxy (omit if none)
PROXY=http://user:pass@host:port

# SERP API (Stage 2+)
SERP_API_KEY=...

# Google Sheets (Stage 3)
GS_JSON=<<one-line JSON>>
GSHEET_ID=xxxxxxxxxxxxxxxxxxxxx

# Twilio (Stage 4)
TWILIO_SID=ACxxxxxxxxxxxx
TWILIO_TOKEN=xxxxxxxxxxxx
MY_WHATSAPP=+447700900000
6. GitHub Actions cron (copy to .github/workflows/scrape.yml)
yaml
Copy
Edit
name: Hack Scraper

on:
  schedule:
    - cron: "0 */6 * * *"       # every 6 h
  workflow_dispatch:            # manual run

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: "3.12" }
      - run: pip install -r requirements.txt
      - run: |
          playwright install --with-deps chromium
          python -m hack_scraper.cli run
        env: ${{ secrets }}      # map repo secrets straight to env
(Render.com or Fly.io one-offs work the same‚Äîbuild the Dockerfile then add a cron schedule.)

7. Two-hour game-plan
‚è∞	Task	Outcome
0‚Äì10 min	Clone scaffold, create virtual-env, install deps, set .env	Local python -m hack_scraper.cli scrape luma returns CSV
10‚Äì45 min	Flesh out Lu.ma scraper (above code)	output/luma.csv with 100-200 rows
45‚Äì70 min	Add Devpost & Eventbrite (static HTML)	CSVs for each, aggregator merges
70‚Äì90 min	Implement exporter.py CSV + Sheets	output/events.csv and sheet populated
90‚Äì110 min	Dockerfile & docker run works; optionally push to GitHub	Container tested locally
110‚Äì120 min	Wire GitHub Action or Render cron	End-to-end scheduled run

What next?
‚úÖ Ready to code? Copy the snippets above and run uvicorn style dev-loop.

‚ùì Need help with Devpost HTML parsing or Google auth JSON? Just shout.

üöÄ Want Firecrawl/agent-style autonomous runs? We can bolt that on after the live demo.

Ping me with whichever bit you‚Äôd like expanded or debugged, and happy hacking! üßÄ







You said:
I want firecrawl as an option and also will need to run a browser eg browserbase


ChatGPT said:

Thought for 26 seconds
Hackathon Scraper Design

Hack Scraper ‚Äì 2‚Äëhour Build Plan
Goal: Collect upcoming hackathons & AI‚Äëevents from multiple public sources, export them, and optionally push notifications ‚Äì all in a single evening sprint.

1. High‚ÄëLevel Architecture
graph TD
  subgraph Scheduler & Runtime
    CRON["Cron / GitHub¬†Action"] --> CONTAINER[Docker Container]
  end

  subgraph Scraper Service
    CONTAINER --> PWR[Playwright¬†+ Proxy]
    PWR --> LUMA[Lu.ma Handler]
    PWR --> DEV[Devpost Handler]
    PWR --> EVT[Eventbrite Handler]
    PWR --> SERP[SERP‚ÄëAPI Handler]
  end

  LUMA --> Q[(Queue / Channel)]
  DEV  --> Q
  EVT  --> Q
  SERP --> Q

  Q --> AGG[Aggregator & De‚Äëduper]
  AGG --> DB[(SQLite / Postgres)]
  AGG --> CSV[CSV Export]
  AGG --> GS[Google¬†Sheets API]
  AGG --> WA[WhatsApp via Twilio]
Everything lives in one repo with a docker-compose.yml so you can hack locally and deploy the same image on Render/Fly/Heroku.

2. Staged Roadmap
Stage	Time	Milestone	What you ship
0	10¬†min	Scaffold	Git repo, virtual‚Äëenv/Poetry, Dockerfile, .env for keys, basic Typer CLI skeleton.
1	40¬†min	Lu.ma AI Calendar	Playwright scraper with infinite scroll üîÑ, date parsing, exports CSV to /output/luma.csv.
2	30¬†min	More Sources	Add Devpost & Eventbrite (simple HTML + query param filters). Introduce aggregator & SQLite.
3	20¬†min	Outputs	Merge ‚Üí events.csv, push to Google Sheets via service account.
4	10¬†min	Proxy & Headless Browser	Swap Playwright launch args for proxy + stealth; parameterise in .env.
5	10¬†min	Cron / Deploy	GitHub Action on schedule or crontab inside container. (e.g. every 6¬†h). Optionally push a WhatsApp summary via Twilio.
(If you still have juice, Stage¬†6 = Firecrawl / Agents.)

3. Core Libraries
Need	Python pkg	Why
Headless browser	playwright (fast), fallback selenium-stealth	Handles JS + infinite scroll
HTTP / fallback	httpx, beautifulsoup4	For static sites
Data wrangling	pandas, python-dateutil	Easy export & date math
Google Sheets	gspread + google‚Äëauth	Sheets API
Messaging	twilio	WhatsApp/SMS push
CLI	typer	Nice DX
Env management	python-dotenv	Secrets in .env
Scheduling	schedule or OS cron	Local runs
Testing	pytest	Quick sanity
4. Example Code Snippets
4.1 Lu.ma AI Calendar Scraper (Playwright)
from playwright.sync_api import sync_playwright
import pandas as pd, datetime as dt

LUMA_URL = "https://lu.ma/ai"

def scrape_luma(limit: int = 300, proxy: str | None = None) -> pd.DataFrame:
    events = []
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=[f"--proxy-server={proxy}"] if proxy else None,
        )
        page = browser.new_page()
        page.goto(LUMA_URL, timeout=60000)
        page.wait_for_selector('[itemprop="event"]')

        while len(events) < limit:
            cards = page.locator('[itemprop="event"]')
            for i in range(cards.count()):
                card = cards.nth(i)
                href = card.locator('a').get_attribute('href')
                title = card.locator('h3').inner_text()
                iso_date = card.locator('time').get_attribute('datetime')
                date = dt.datetime.fromisoformat(iso_date)
                events.append({"title": title, "url": href, "date": date})
            # trigger more content
            page.mouse.wheel(0, 2000)
            page.wait_for_timeout(1200)
            if page.locator('text="No more"').count():
                break
        browser.close()
    return pd.DataFrame(events)
4.2 SERP‚ÄëAPI Query
import os, requests, pandas as pd

def google_hackathons(query="hackathon", location=None, api_key=os.getenv("SERP_API_KEY")):
    params = {
        "engine": "google",
        "q": f"{query} site:lu.ma | site:eventbrite.com | site:devpost.com",
        "api_key": api_key,
        "num": 100,
    }
    if location:
        params["location"] = location
    res = requests.get("https://serpapi.com/search", params=params, timeout=15).json()
    return pd.DataFrame(res.get("organic_results", []))
4.3 Aggregator Pseudocode
for source in sources:
    df = source.scrape()
    upsert(df)  # keyed on canonical_url

rows = db.read("SELECT * FROM events WHERE date >= today")
export_csv(rows)
export_google_sheets(rows)
if new_items:
    send_whatsapp(summary(new_items))
5. Deployment Cheatsheet
Dockerfile (python:3.12‚Äëslim) installs system deps: playwright install chromium.

docker‚Äëcompose.yml ‚Äì mount /output for CSV & .env secrets.

Render.com: one‚Äëoff cron job ‚Üí python main.py run.

GitHub Actions: workflow cron ‚Üí checkout ‚Üí setup‚Äëpython ‚Üí pip install -r reqs ‚Üí python main.py run.

Fly.io / Heroku as Plan¬†B.

6. Proxy & Stealth Notes
Use BrightData or ProxyMesh ‚Üí store creds in .env.

In Playwright: launch(args=["--proxy-server=http://user:pass@host:port"]).

For truly nasty sites add playwright-stealth.

7. WhatsApp Notifier (Twilio)
from twilio.rest import Client

def notify_whatsapp(msg: str):
    client = Client(os.getenv('TWILIO_SID'), os.getenv('TWILIO_TOKEN'))
    client.messages.create(
        body=msg,
        from_='whatsapp:+14155238886',
        to=os.getenv('MY_WHATSAPP')
    )
Call this from the aggregator whenever new_items is non‚Äëempty.

8. Reference Command‚ÄëLine
# Run scrape locally
python main.py scrape --source luma --limit 200

# Aggregate & export
python main.py aggregate --sheet hacks

# Full pipeline (for cron)
python main.py run
9. Firecrawl & Browserbase (optional bolt‚Äëons)
9.1¬†Firecrawl
When to use¬†‚Äì lightning‚Äëfast static scraping when you don‚Äôt need full JS rendering.

Install¬†pip install firecrawl

Env¬†FIRECRAWL_API_KEY=<token>

from firecrawl import Firecrawl
import os

fc = Firecrawl(api_key=os.getenv("FIRECRAWL_API_KEY"))

def fc_scrape(url: str):
    data = fc.crawl(url, extract=["title", "html", "images", "date_published"])
    return {
        "title": data.get("title"),
        "url": url,
        "date": data.get("date_published"),
        "source": "firecrawl",
    }
Wrap this in a FirecrawlScraper(BaseScraper) and register it in scraper/__init__.py so --mode firecrawl works across any URL list.

9.2¬†Browserbase ‚Äì remote Chromium
Why¬†‚Äì off‚Äëload CPU/RAM, evade local network limits, scale parallel tabs.

Install client¬†pip install browserbase

Env

BROWSERBASE_API_KEY

BROWSERBASE_REGION (optional)

from browserbase import BrowserBase
from playwright.async_api import async_playwright
import os, asyncio

async def bb_launch():
    bb = BrowserBase(os.getenv("BROWSERBASE_API_KEY"))
    ws_endpoint = bb.create_session().get("ws_endpoint")
    p = await async_playwright().start()
    browser = await p.chromium.connect_over_cdp(ws_endpoint)
    return browser
Add a USE_BROWSERBASE=true guard inside BaseScraper so you transparently swap launch() for connect_over_cdp.

9.3¬†Runtime modes summary
Mode	Trigger	Renderer	Best for
local (default)	nothing	Playwright local	fast dev loops
proxy	PROXY= set	Playwright + proxy	geo‚Äëblocked sites
browserbase	USE_BROWSERBASE=true	Remote Chromium	heavy/parallel crawls
firecrawl	FIRECRAWL_API_KEY	Firecrawl API	pure HTML pages
typer CLI addition:

python -m hack_scraper.cli scrape luma --mode browserbase
Use an Enum for --mode and let each scraper decide the launch strategy.

10.¬†Good luck & happy hacking! üßÄ


Edited

Answer in chat instead
‚úÖ Added Section 9 to the blueprint with:

Firecrawl integration ‚Äì install, env var, helper snippet, and how to wrap it as a FirecrawlScraper.

Browserbase remote browser ‚Äì quick client example and switchable launch logic.

A modes table + CLI flag so you can flip between local, proxy, browserbase, and firecrawl.

Open the canvas to see the full diff, and let me know which piece you‚Äôd like fleshed out next‚Äîe.g., wiring the --mode flag, Devpost via Firecrawl, or Browserbase parallelization.

# Hack Scraper ‚Äì 2‚Äëhour Build Plan

> **Goal:** Collect upcoming hackathons & AI‚Äëevents from multiple public sources, export them, and optionally push notifications ‚Äì all in a single evening sprint.

---

## 1. High‚ÄëLevel Architecture

```mermaid
graph TD
  subgraph Scheduler & Runtime
    CRON["Cron / GitHub¬†Action"] --> CONTAINER[Docker Container]
  end

  subgraph Scraper Service
    CONTAINER --> PWR[Playwright¬†+ Proxy]
    PWR --> LUMA[Lu.ma Handler]
    PWR --> DEV[Devpost Handler]
    PWR --> EVT[Eventbrite Handler]
    PWR --> SERP[SERP‚ÄëAPI Handler]
  end

  LUMA --> Q[(Queue / Channel)]
  DEV  --> Q
  EVT  --> Q
  SERP --> Q

  Q --> AGG[Aggregator & De‚Äëduper]
  AGG --> DB[(SQLite / Postgres)]
  AGG --> CSV[CSV Export]
  AGG --> GS[Google¬†Sheets API]
  AGG --> WA[WhatsApp via Twilio]
```

*Everything lives in **one repo** with a `docker-compose.yml` so you can hack locally and deploy the same image on Render/Fly/Heroku.*

---

## 2. Staged Roadmap

| Stage | Time   | Milestone                    | What you ship                                                                                                             |
| ----- | ------ | ---------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| **0** | 10¬†min | **Scaffold**                 | Git repo, virtual‚Äëenv/Poetry, Dockerfile, `.env` for keys, basic `Typer` CLI skeleton.                                    |
| **1** | 40¬†min | **Lu.ma AI Calendar**        | Playwright scraper with infinite scroll üîÑ, date parsing, exports CSV to `/output/luma.csv`.                              |
| **2** | 30¬†min | **More Sources**             | Add Devpost & Eventbrite (simple HTML + query param filters). Introduce aggregator & SQLite.                              |
| **3** | 20¬†min | **Outputs**                  | Merge ‚Üí `events.csv`, push to Google Sheets via service account.                                                          |
| **4** | 10¬†min | **Proxy & Headless Browser** | Swap Playwright launch args for proxy + stealth; parameterise in `.env`.                                                  |
| **5** | 10¬†min | **Cron / Deploy**            | GitHub Action on schedule or `crontab` inside container. (e.g. every 6¬†h). Optionally push a WhatsApp summary via Twilio. |

*(If you still have juice, Stage¬†6 = Firecrawl / Agents.)*

---

## 3. Core Libraries

| Need             | Python pkg                                       | Why                          |
| ---------------- | ------------------------------------------------ | ---------------------------- |
| Headless browser | `playwright` (fast), fallback `selenium-stealth` | Handles JS + infinite scroll |
| HTTP / fallback  | `httpx`, `beautifulsoup4`                        | For static sites             |
| Data wrangling   | `pandas`, `python-dateutil`                      | Easy export & date math      |
| Google Sheets    | `gspread` + `google‚Äëauth`                        | Sheets API                   |
| Messaging        | `twilio`                                         | WhatsApp/SMS push            |
| CLI              | `typer`                                          | Nice DX                      |
| Env management   | `python-dotenv`                                  | Secrets in .env              |
| Scheduling       | `schedule` or OS cron                            | Local runs                   |
| Testing          | `pytest`                                         | Quick sanity                 |

---

## 4. Example Code Snippets

### 4.1 Lu.ma AI Calendar Scraper (Playwright)

```python
from playwright.sync_api import sync_playwright
import pandas as pd, datetime as dt

LUMA_URL = "https://lu.ma/ai"

def scrape_luma(limit: int = 300, proxy: str | None = None) -> pd.DataFrame:
    events = []
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=[f"--proxy-server={proxy}"] if proxy else None,
        )
        page = browser.new_page()
        page.goto(LUMA_URL, timeout=60000)
        page.wait_for_selector('[itemprop="event"]')

        while len(events) < limit:
            cards = page.locator('[itemprop="event"]')
            for i in range(cards.count()):
                card = cards.nth(i)
                href = card.locator('a').get_attribute('href')
                title = card.locator('h3').inner_text()
                iso_date = card.locator('time').get_attribute('datetime')
                date = dt.datetime.fromisoformat(iso_date)
                events.append({"title": title, "url": href, "date": date})
            # trigger more content
            page.mouse.wheel(0, 2000)
            page.wait_for_timeout(1200)
            if page.locator('text="No more"').count():
                break
        browser.close()
    return pd.DataFrame(events)
```

### 4.2 SERP‚ÄëAPI Query

```python
import os, requests, pandas as pd

def google_hackathons(query="hackathon", location=None, api_key=os.getenv("SERP_API_KEY")):
    params = {
        "engine": "google",
        "q": f"{query} site:lu.ma | site:eventbrite.com | site:devpost.com",
        "api_key": api_key,
        "num": 100,
    }
    if location:
        params["location"] = location
    res = requests.get("https://serpapi.com/search", params=params, timeout=15).json()
    return pd.DataFrame(res.get("organic_results", []))
```

### 4.3 Aggregator Pseudocode

```text
for source in sources:
    df = source.scrape()
    upsert(df)  # keyed on canonical_url

rows = db.read("SELECT * FROM events WHERE date >= today")
export_csv(rows)
export_google_sheets(rows)
if new_items:
    send_whatsapp(summary(new_items))
```

---

## 5. Deployment Cheatsheet

1. **Dockerfile** (python:3.12‚Äëslim) installs system deps: `playwright install chromium`.
2. **docker‚Äëcompose.yml** ‚Äì mount `/output` for CSV & `.env` secrets.
3. **Render.com**: one‚Äëoff cron job ‚Üí `python main.py run`.
4. **GitHub Actions**: workflow cron ‚Üí checkout ‚Üí setup‚Äëpython ‚Üí `pip install -r reqs` ‚Üí `python main.py run`.
5. **Fly.io / Heroku** as Plan¬†B.

---

## 6. Proxy & Stealth Notes

* Use **BrightData** or **ProxyMesh** ‚Üí store creds in `.env`.
* In Playwright: `launch(args=["--proxy-server=http://user:pass@host:port"])`.
* For truly nasty sites add [`playwright-stealth`](https://github.com/msidorenko/playwright-stealth).

---

## 7. WhatsApp Notifier (Twilio)

```python
from twilio.rest import Client

def notify_whatsapp(msg: str):
    client = Client(os.getenv('TWILIO_SID'), os.getenv('TWILIO_TOKEN'))
    client.messages.create(
        body=msg,
        from_='whatsapp:+14155238886',
        to=os.getenv('MY_WHATSAPP')
    )
```

Call this from the aggregator whenever `new_items` is non‚Äëempty.

---

## 8. Reference Command‚ÄëLine

```bash
# Run scrape locally
python main.py scrape --source luma --limit 200

# Aggregate & export
python main.py aggregate --sheet hacks

# Full pipeline (for cron)
python main.py run
```

---

---

## 9. Firecrawl & Browserbase (optional bolt‚Äëons)

### 9.1¬†Firecrawl

* **When to use**¬†‚Äì lightning‚Äëfast static scraping when you don‚Äôt need full JS rendering.
* **Install**¬†`pip install firecrawl`
* **Env**¬†`FIRECRAWL_API_KEY=<token>`

```python
from firecrawl import Firecrawl
import os

fc = Firecrawl(api_key=os.getenv("FIRECRAWL_API_KEY"))

def fc_scrape(url: str):
    data = fc.crawl(url, extract=["title", "html", "images", "date_published"])
    return {
        "title": data.get("title"),
        "url": url,
        "date": data.get("date_published"),
        "source": "firecrawl",
    }
```

*Wrap this in a `FirecrawlScraper(BaseScraper)` and register it in `scraper/__init__.py` so `--mode firecrawl` works across any URL list.*

---

### 9.2¬†Browserbase ‚Äì remote Chromium

* **Why**¬†‚Äì off‚Äëload CPU/RAM, evade local network limits, scale parallel tabs.
* **Install client**¬†`pip install browserbase`
* **Env**

  * `BROWSERBASE_API_KEY`
  * `BROWSERBASE_REGION` (optional)

```python
from browserbase import BrowserBase
from playwright.async_api import async_playwright
import os, asyncio

async def bb_launch():
    bb = BrowserBase(os.getenv("BROWSERBASE_API_KEY"))
    ws_endpoint = bb.create_session().get("ws_endpoint")
    p = await async_playwright().start()
    browser = await p.chromium.connect_over_cdp(ws_endpoint)
    return browser
```

*Add a `USE_BROWSERBASE=true` guard inside `BaseScraper` so you transparently swap `launch()` for `connect_over_cdp`.*

---

### 9.3¬†Runtime modes summary

| Mode                | Trigger                | Renderer           | Best for              |
| ------------------- | ---------------------- | ------------------ | --------------------- |
| **local** (default) | nothing                | Playwright local   | fast dev loops        |
| **proxy**           | `PROXY=` set           | Playwright + proxy | geo‚Äëblocked sites     |
| **browserbase**     | `USE_BROWSERBASE=true` | Remote Chromium    | heavy/parallel crawls |
| **firecrawl**       | `FIRECRAWL_API_KEY`    | Firecrawl API      | pure HTML pages       |

`typer` CLI addition:

```bash
python -m hack_scraper.cli scrape luma --mode browserbase
```

Use an `Enum` for `--mode` and let each scraper decide the launch strategy.

---

### 10.¬†Good luck & happy hacking! üßÄ
